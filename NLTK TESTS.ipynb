{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRILLTagger\n",
    "http://www.nltk.org/api/nltk.tag.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tag import untag, RegexpTagger, BrillTaggerTrainer\n",
    "from nltk.corpus import  conll2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'NC'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [ tag for (word, tag) in conll2002.tagged_words('esp.train') ]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8323\n",
      "[(u'Melbourne', u'NP'), (u'(', u'Fpa'), (u'Australia', u'NP'), (u')', u'Fpt'), (u',', u'Fc'), (u'25', u'Z'), (u'may', u'NC'), (u'(', u'Fpa'), (u'EFE', u'NC'), (u')', u'Fpt'), (u'.', u'Fp')]\n",
      "35651\n",
      "[(u'Sao', u'NC'), (u'Paulo', u'VMI'), (u'(', u'Fpa'), (u'Brasil', u'NC'), (u')', u'Fpt'), (u',', u'Fc'), (u'23', u'Z'), (u'may', u'NC'), (u'(', u'Fpa'), (u'EFECOM', u'NP'), (u')', u'Fpt'), (u'.', u'Fp')]\n"
     ]
    }
   ],
   "source": [
    "train_senteces = conll2002.tagged_sents('esp.train')\n",
    "print len(train_senteces)\n",
    "print train_senteces[0]\n",
    "all_senteces = conll2002.tagged_sents()\n",
    "print len(all_senteces)\n",
    "print all_senteces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = train_senteces[:-100]\n",
    "# baseline_data = senteces[-400:-100] #equivalent to CV data\n",
    "gold_data = train_senteces[-100:]  #equivalent to test data\n",
    "testing_data = [untag(s) for s in gold_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Unigram Tagger\n",
      "- Done.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tbl.template import Template\n",
    "from nltk.tag.brill import Pos, Word\n",
    "from nltk.tag import DefaultTagger\n",
    "\n",
    "default_tagger = DefaultTagger('NC')\n",
    "\n",
    "from nltk.tag import UnigramTagger\n",
    "\n",
    "print \"- Unigram Tagger\"\n",
    "unigram_tagger = UnigramTagger(training_data,backoff=default_tagger)\n",
    "print \"- Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Templates\n",
      "- Creating Trainer\n",
      "- Begin Training\n",
      "TBL train (fast) (seqs: 8223; tokens: 261562; tpls: 5; min score: 2; min acc: None)\n",
      "Finding initial useful rules...\n",
      "    Found 10051 useful rules.\n",
      "Selecting rules...\n",
      "- Done.\n"
     ]
    }
   ],
   "source": [
    "print \"- Templates\"\n",
    "Template._cleartemplates() \n",
    "templates = [\n",
    "    Template(Pos([-1])), \n",
    "    Template(Pos([-1]), Word([0])), \n",
    "    Template(Pos([-2])), \n",
    "    Template(Pos([-2]), Word([0])),\n",
    "    Template(Pos([1])), \n",
    "]\n",
    "print \"- Creating Trainer\"\n",
    "tt = BrillTaggerTrainer(unigram_tagger, templates, trace=1)\n",
    "print \"- Begin Training\"\n",
    "tagger1 = tt.train(training_data, max_rules=1000)\n",
    "print \"- Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9381541389153187"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger1.evaluate(gold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple stupid test... that resulted in something \n",
    "Try to use the iob_sents to find the Named Entity:\n",
    "\n",
    "1. Extract features\n",
    "   - First letter uppercase?\n",
    "   - Begin of chunk\n",
    "   - pos[-1] tag\n",
    "   - pos[1] tag\n",
    "   - pos in chunk\n",
    "   - pos in sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alextabora/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_df(sentences):\n",
    "    s_id = []\n",
    "    s_word = []\n",
    "    s_tag = []\n",
    "    s_iob =[]\n",
    "    s_pos = []\n",
    "\n",
    "    for sent_num in range(len(sentences)):\n",
    "        sent = sentences[sent_num]\n",
    "        for pos in range(len(sent)):\n",
    "            word = sent[pos]\n",
    "            s_id.append(sent_num)\n",
    "            s_word.append(word[0])\n",
    "            s_tag.append(word[1])\n",
    "            s_iob.append(word[2])\n",
    "            s_pos.append(pos)\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "            \"sentence\": s_id,\n",
    "            \"word\": s_word,\n",
    "            \"tag\": s_tag,\n",
    "            \"iob\": s_iob,\n",
    "            \"pos\": s_pos\n",
    "        })\n",
    "    \n",
    "    return df\n",
    "sent_df = get_df(conll2002.iob_sents('esp.train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'B-LOC', u'B-MISC', u'B-ORG', u'B-PER', u'I-LOC', u'I-MISC', u'I-ORG', u'I-PER', u'O']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iob</th>\n",
       "      <th>tag</th>\n",
       "      <th>upper</th>\n",
       "      <th>pos</th>\n",
       "      <th>first</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iob  tag  upper  pos  first  size\n",
       "0    0   28   True    0      1     9\n",
       "1    8   20  False    1      0     1\n",
       "2    0   28   True    2      0     9\n",
       "3    8   21  False    3      0     1\n",
       "4    8   12  False    4      0     1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_features(base_df):\n",
    "    df = pd.DataFrame()\n",
    "    # iob\n",
    "    le_iob = LabelEncoder()\n",
    "    df.loc[:,\"iob\"] = le_iob.fit_transform(base_df.iob)\n",
    "    print list(le_iob.classes_)\n",
    "\n",
    "    #tag\n",
    "    le_tag = LabelEncoder()\n",
    "    df.loc[:,\"tag\"] = le_tag.fit_transform(base_df.tag)\n",
    "#     print list(le_tag.classes_)\n",
    "    \n",
    "    # Uppercase\n",
    "    df.loc[:,\"upper\"] = base_df.word.apply(lambda x: x[0].isupper())\n",
    "    \n",
    "    # Pos\n",
    "    df.loc[:,\"pos\"] = base_df.pos\n",
    "    \n",
    "    #first \n",
    "    df.loc[:,\"first\"] = base_df.pos.apply(lambda x: int(x == 0) )\n",
    "    #size\n",
    "    df.loc[:,\"size\"] = base_df.word.apply(lambda x: len(x))\n",
    "    \n",
    "    \n",
    "    return df, le_iob, le_tag\n",
    "\n",
    "\n",
    "\n",
    "feature_df, le_iob, le_tag = get_features(sent_df)\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alextabora/anaconda/lib/python2.7/site-packages/pandas/core/generic.py:2773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    231920\n",
       "1     32795\n",
       "Name: iob, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#balancer\n",
    "def balance_classes(df):\n",
    "\n",
    "    classes = []\n",
    "    for i in range(9):\n",
    "        #binarizing\n",
    "        if i == 8:\n",
    "            class_df = df[df[\"iob\"]==i]#[:75170]\n",
    "            class_df.iob = 0\n",
    "            classes.append(class_df )\n",
    "        else:\n",
    "            class_df = df[df[\"iob\"]==i]#[:14000]\n",
    "            class_df.iob = 1\n",
    "            classes.append(class_df)\n",
    "            \n",
    "\n",
    "    newdf = pd.concat(classes)\n",
    "        \n",
    "    return newdf\n",
    "\n",
    "neo_feature_df = balance_classes(feature_df)\n",
    "neo_feature_df.iob.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getXY(df):\n",
    "    X = df.drop(\"iob\",1).values\n",
    "    y = df.iob.values\n",
    "    \n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X,y = getXY(neo_feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "0.978740193836\n",
      "0.975899063161\n",
      "---------\n",
      "0.97847995534\n",
      "0.978165608945\n",
      "---------\n",
      "0.978677232909\n",
      "0.976692354186\n",
      "---------\n",
      "0.978698219885\n",
      "0.975974614687\n",
      "---------\n",
      "0.978576495427\n",
      "0.977296766395\n",
      "---------\n",
      "0.978794848978\n",
      "0.975709266745\n",
      "---------\n",
      "0.978673125031\n",
      "0.976351478977\n",
      "---------\n",
      "0.978563993217\n",
      "0.977673680632\n",
      "---------\n",
      "0.978635348634\n",
      "0.977484794681\n",
      "---------\n",
      "0.978538808952\n",
      "0.978013675343\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state= 233, n_jobs=2)\n",
    "\n",
    "#train\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10,shuffle=True, random_state= 233)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    #train\n",
    "    clf.fit(X_train, y_train)\n",
    "    print \"---------\"\n",
    "    print clf.score(X_train,y_train)\n",
    "    print clf.score(X_test,y_test)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97801367534282801"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "preds = clf.predict(X_test)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23037,   155],\n",
       "       [  427,  2852]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         NE       0.98      0.99      0.99     23192\n",
      "      Other       0.95      0.87      0.91      3279\n",
      "\n",
      "avg / total       0.98      0.98      0.98     26471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['NE', 'Other']\n",
    "print(classification_report(y_test, preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagged Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- train all\n",
      "- Done.\n"
     ]
    }
   ],
   "source": [
    "#train all\n",
    "print \"- train all\"\n",
    "clf_final = RandomForestClassifier(n_estimators=100, random_state= 233, n_jobs=2)\n",
    "clf_final.fit(X, y)\n",
    "print \"- Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Debido', u'AQ'),\n",
       " ('a', u'SP'),\n",
       " ('problemas', 'NC'),\n",
       " ('en', u'SP'),\n",
       " ('el', u'DA'),\n",
       " ('sistema', 'NC'),\n",
       " ('interno', u'AQ'),\n",
       " ('que', u'PR'),\n",
       " ('lleva', u'VMI'),\n",
       " ('el', u'DA'),\n",
       " ('Hospital', 'NC'),\n",
       " ('Escuela,', 'NC'),\n",
       " ('las', u'DA'),\n",
       " ('citas', 'NC'),\n",
       " ('est\\xc3\\xa1n', 'NC'),\n",
       " ('siendo', u'VSG'),\n",
       " ('programadas', u'AQ'),\n",
       " ('para', u'SP'),\n",
       " ('el', u'DA'),\n",
       " ('2010', u'Z')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"El conductor del carro fue identificado como Miguel Perez que conducía en Brazil\"\n",
    "\n",
    "raw = \"De conformidad con las leyes vigentes se realizaron los estudios en San Pedro Sula\"\n",
    "raw = \"Debido a problemas en el sistema interno que lleva el Hospital Escuela, las citas están siendo programadas para el 2010\"\n",
    "\n",
    "\n",
    "tagged_test = tagger1.tag(raw.split())\n",
    "tagged_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, True, 0, 1, 6],\n",
       " [39, False, 1, 0, 1],\n",
       " [27, False, 2, 0, 9],\n",
       " [39, False, 3, 0, 2],\n",
       " [4, False, 4, 0, 2],\n",
       " [27, False, 5, 0, 7],\n",
       " [1, False, 6, 0, 7],\n",
       " [34, False, 7, 0, 3],\n",
       " [46, False, 8, 0, 5],\n",
       " [4, False, 9, 0, 2],\n",
       " [27, True, 10, 0, 8],\n",
       " [27, True, 11, 0, 8],\n",
       " [4, False, 12, 0, 3],\n",
       " [27, False, 13, 0, 5],\n",
       " [27, False, 14, 0, 6],\n",
       " [51, False, 15, 0, 6],\n",
       " [1, False, 16, 0, 11],\n",
       " [39, False, 17, 0, 4],\n",
       " [4, False, 18, 0, 2],\n",
       " [58, False, 19, 0, 4]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert tagged sentence into X format\n",
    "def get_X_real(tagged_sent):\n",
    "    X_real = []\n",
    "    for word_num in range(len(tagged_sent)):\n",
    "        #tag\n",
    "        tag = le_tag.transform([tagged_sent[word_num][1]])[0]\n",
    "        #upper\n",
    "        upper = tagged_sent[word_num][0][0].isupper()\n",
    "        #pos\n",
    "        pos = word_num\n",
    "        #first\n",
    "        first = int(pos==0)\n",
    "        #size\n",
    "        size = len(tagged_sent[word_num][0])\n",
    "\n",
    "        X_real.append([tag,upper,pos,first,size])\n",
    "    return X_real\n",
    "    \n",
    "X_real = get_X_real(tagged_test)\n",
    "X_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debido\n",
      "Hospital\n",
      "Escuela,\n"
     ]
    }
   ],
   "source": [
    "preds_real = clf_final.predict(X_real)\n",
    "for i in range(len(preds_real)):\n",
    "    if preds_real[i] == 1:\n",
    "        print tagged_test[i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Sentence 0 \n",
      "TEGUCIGALPA\n",
      "\n",
      "TEGUCIGALPA\n",
      "\n",
      "-- Sentence 1 \n",
      "- Una misteriosa obstrucción del sistema de archivos y citas del Hospital Escuela, ha generado un caos entre los pacientes, debido a que todas las citas están siendo programadas hasta para cuatro meses del próximo año 2010 según las denuncias de los mismos empleados\n",
      "\n",
      "Hospital\n",
      "Escuela,\n",
      "\n",
      "-- Sentence 2 \n",
      "\n",
      "Debido a problemas en el sistema interno que lleva el Hospital Escuela, las citas están siendo programadas para el 2010\n",
      "\n",
      "Debido\n",
      "Hospital\n",
      "Escuela,\n",
      "\n",
      "-- Sentence 3 \n",
      "\n",
      "Carmen Linares empleada del este centro hospitalario y afectada denunció el problema en el que se le están suspendiendo las citas a los pacientes, incluso hay rótulos de notificaciones anunciando que hasta segundo aviso se restablecerán están servicios\n",
      "\n",
      "Carmen\n",
      "Linares\n",
      "\n",
      "-- Sentence 4 \n",
      "\n",
      "“Nosotros estamos teniendo problemas con las citas, porque de forma extraña se dañaron las computadoras en donde se lleva el registro de las citas y los pacientes son los que están sufriendo las consecuencias”, expresó Linares\n",
      "\n",
      "Linares\n",
      "\n",
      "-- Sentence 5 \n",
      "\n",
      "La denunciante lamentó que hay tanta gente del interior del país que madruga a cumplir con sus citas para continuar los tratamientos de sus padecimientos, pero lastimosamente llegan a nada más que a gastar sus pocos recursos económicos\n",
      "\n",
      "\n",
      "-- Sentence 6 \n",
      "\n",
      "Linares indicó que desde hace varios días se dañó el sistema computarizado para tabular el registro de citas para pacientes que llevan un tratamiento con los especialistas, sin embargo se desconoce cuándo será restablecido\n",
      "\n",
      "Linares\n",
      "\n",
      "-- Sentence 7 \n",
      "\n",
      "De esta forma nuevamente se vuelva a retrasar el proceso de citas después que en el mes de noviembre pasado fueron reprogramadas por más de 15 días previo a las elecciones presidenciales, y ahora se retrasan y con esto se incrementa la mora de atenciones\n",
      "\n",
      "\n",
      "-- Sentence 8 \n",
      " \n",
      "El director de este complejo hospitalario, Juan Ramón Barahona, manifestó que ya conoció de del problema y iniciaran las investigaciones sobre el asunto y a la vez buscar la solución al mismo, para no retrasar el proceso de cita de los paciente\n",
      "\n",
      "Juan\n",
      "Ramón\n",
      "Barahona,\n",
      "\n",
      "-- Sentence 9 \n",
      "\n",
      "Barahona agregó que el equipo computarizado del departamento de archivo y citas, había sido saturado en una oportunidad, pero estuvo funcionado con normalidad, sin embargo nuevamente se le reportó de algunos desperfectos técnicos que ha generado la suspensión de algunas citas a pacientes\n",
      "\n",
      "Barahona\n",
      "\n",
      "-- Sentence 10 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "import bs4 #beautiful soup\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "articles = pd.read_csv(\"./files/articles.csv\")\n",
    "articles.shape\n",
    "\n",
    "content = articles.loc[1].content   #1\n",
    "soup = bs4.BeautifulSoup(content, 'html.parser')\n",
    "# print soup.get_text()\n",
    "\n",
    "raw = soup.get_text()\n",
    "sentences = raw.split(\".\")\n",
    "#Tag and prepare\n",
    "for index, sent in enumerate(sentences):\n",
    "    \n",
    "    print \n",
    "    print \"-- Sentence %i \" %index\n",
    "    print sent\n",
    "    print\n",
    "    \n",
    "    if sent ==\"\\n\":\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    tagged_sent = tagger1.tag( sent.split() )\n",
    "#     print tagged_sent\n",
    "    X_real = get_X_real(tagged_sent)\n",
    "\n",
    "\n",
    "    #predict \n",
    "    preds_real = clf.predict(X_real)\n",
    "#     print preds_real\n",
    "    for i in range(len(preds_real)):\n",
    "        if preds_real[i] == 1:\n",
    "            print tagged_sent[i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-Conclusion...\n",
    "\n",
    "### RFC can be used for Name Entitity identification task, given a POS tagged ... Although in practice it works fairly wel there are some cases it doesn't correctly identify.\n",
    "\n",
    "## Next Steps\n",
    "- Improve the POS tagger\n",
    "- Improve the NE identifier\n",
    "- Train a NER Classifier (one that classifies the identified NE )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reviewing how the sentences are tagged\n",
    "# sent_num = 6\n",
    "\n",
    "# print \" \".join(conll2002.sents()[sent_num])\n",
    "# print\n",
    "# print conll2002.tagged_sents()[sent_num]\n",
    "\n",
    "#find a sentence that starts with a \"Art\"\n",
    "for index, sent in enumerate(conll2002.tagged_sents('esp.train')):\n",
    "    if sent[0][1] == u\"Art\":\n",
    "        print index\n",
    "        print sent\n",
    "        \n",
    "        \n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(conll2002.iob_sents())\n",
    "print len(conll2002.iob_sents('esp.testa'))\n",
    "conll2002.iob_sents()[11758]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df.iob.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df.iob.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
